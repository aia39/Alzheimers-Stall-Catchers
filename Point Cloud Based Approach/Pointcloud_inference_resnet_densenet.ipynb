{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pointcloud_inference_resnet_densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRiZZDvOSJb",
        "colab_type": "text"
      },
      "source": [
        "# **Alzheimer-Stall-Catchers-Point-Cloud**\n",
        "\n",
        "This notebook contains necessary code for inference of test data explained in **Point Cloud Based Approach** using 3D convolutional models such as:\n",
        "- Resnet3D 18\n",
        "- Resnet3D 101, 152, 200\n",
        "- Densenet3D 121, 169, 201, 264"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMnMdYzYiThY",
        "colab_type": "text"
      },
      "source": [
        "### Test GPU\n",
        "\n",
        "This portion is to test available GPU on the machine. Torch models run on CUDA enabled devices, and it is recommended to use such a machine. In case of running the code on Google Colab, after connecting to a new session, be sure to test which GPU is provided for the session. **Tesla K80** is the slowest GPU that will take 5-6 times training time than **Tesla T4** or **Tesla P100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rftLe66DQIw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-woKEfviSEf",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive \n",
        "\n",
        "**Skip this step if running on local machine**\n",
        "\n",
        "Running the notebook on Colab requires data files and custom python modules copied from google drive or uploading the files to the colab session. However due to large dataset sizes, uploading data on each session if not a viable option. It is recommended to upload the data on a google drive, and running the following cell, allow file access to that drive for easily copying necessary files.\n",
        "\n",
        "(Note that free google drive accounts give you only 15 GB of storage. In case data volume is bigger than that, you can create segments of the total data, hosting them on different drives, and then simply add shortcuts of the different drive folders to a single drive, giving you access to all the data from one google drive.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2etPrrlvR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha8XOZ-9PYnS",
        "colab_type": "text"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiVLFMGkvWdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef as mcc\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.manual_seed(100)\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSE5uxfFPflT",
        "colab_type": "text"
      },
      "source": [
        "# Copy data\n",
        "\n",
        "**Skip this cell if running on local machine**\n",
        "\n",
        "After mounting google drive, you need to copy the necessary custom python modules, dataset and other files. The following cell first copies that large zipped data, and extracts them into the current colab workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX_jwU4Wva_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aba26d72-38c2-4bcc-9ab3-bfdfd20be5ce"
      },
      "source": [
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/resnet.py\", \"resnet.py\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/densenet.py\", \"densenet.py\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/submission_format.csv\", \"submission_format.csv\")\n",
        "\n",
        "\n",
        "submission_format_csv = \"submission_format.csv\"\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as-f0hnidHLb",
        "colab_type": "text"
      },
      "source": [
        "If you have not yet converted the original dataset to the point cloud dataset yet, head to the <a href=\"https://github.com/ClockWorkKid/Alzheimers-Stall-Catchers/tree/master/Dataset%20Visualization%20and%20Processing\">**DATA VISUALIZATION AND PROCESSING**</a> section of the repository\n",
        "\n",
        "\n",
        "For importing test dataset from google drive, the data has been partitioned into 15 zip files, and each of the files are extracted into the colab session one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TpKd0izZwzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"This section is going to take about 20 minutes time\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_1.zip\";\n",
        "print(\"partition 1 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_2.zip\";\n",
        "print(\"partition 2 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_3.zip\";\n",
        "print(\"partition 3 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_4.zip\";\n",
        "print(\"partition 4 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_5.zip\";\n",
        "print(\"partition 5 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_6.zip\";\n",
        "print(\"partition 6 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_7.zip\";\n",
        "print(\"partition 7 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_8.zip\";\n",
        "print(\"partition 8 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_9.zip\";\n",
        "print(\"partition 9 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_10.zip\";\n",
        "print(\"partition 10 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_11.zip\";\n",
        "print(\"partition 11 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_12.zip\";\n",
        "print(\"partition 12 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_13.zip\";\n",
        "print(\"partition 13 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_14.zip\";\n",
        "print(\"partition 14 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/test_15.zip\";\n",
        "print(\"partition 15 imported\")\n",
        "\n",
        "path=\"./test/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIrezS0QTv2",
        "colab_type": "text"
      },
      "source": [
        "Alzheimer Stall Catchers **Test** dataset contains 14160 data samples, and there is a sanity check to see whether all the data files have been imported successfully. If running on local machine, you can just specify the data folder to check if all files are there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtbHtkuaQZEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = [f for f in glob.glob(\"test/\" + \"*\" + \".pt\", recursive=True)]\n",
        "print(\"Total: \" + str(len(files)) + \" should be 14160\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqt_1GtcoMD",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58VoWWG1cp9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string2torch(test_y):\n",
        "  df = pd.DataFrame(test_y, columns = ['Fname'])\n",
        "  y = (df['Fname'].values)\n",
        "\n",
        "  for i,filename in enumerate(y):\n",
        "    seq_name = filename.split(\".mp4\")[0]\n",
        "    y[i] = seq_name\n",
        "\n",
        "  processed = np.array(y)\n",
        "  processed = processed.astype(np.int)\n",
        "  processed = torch.from_numpy(processed)\n",
        "\n",
        "  return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHVrDSS5ctYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_submission_file(filenames, y_pred):\n",
        "  submit = []\n",
        "  filenames = filenames.astype(int)\n",
        "  for i in filenames:\n",
        "    submit += [str(i)+'.mp4']\n",
        "\n",
        "  submission_dict = {\"filename\": submit, \"stalled\": np.round(y_pred,3)}\n",
        "  submission_csv = pd.DataFrame(submission_dict)\n",
        "  submission_csv.to_csv(\"submission3D.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFossLqLcwoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(submission_format_csv, mode='r') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    test_list_csv = {rows[0]: rows[1] for rows in reader}\n",
        "    infile.close()\n",
        "\n",
        "\n",
        "files = list(test_list_csv.keys())\n",
        "files.pop(0)\n",
        "test_len = len(files)\n",
        "print(\"Total data: \" + str(test_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro6BzS5XcBIF",
        "colab_type": "text"
      },
      "source": [
        "# **Network Model**\n",
        "\n",
        "As mentioned at the beginning of the notebook, Resnet3D/Densenet3D models have been used for training. The model must be imported and sent to device prior to training, and depending on which model you wish to train, you have to import that model either from torch libary (resnet3D 18) or our custorm python modules (resnet.py/densenet.py)\n",
        "\n",
        "###**Resnet3D 18**\n",
        "```\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "model = r3d_18(pretrained = False) # Change to true for a pretrained model \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.fc.out_features = 2\n",
        "```\n",
        "\n",
        "###**Resnet 101, 152, 200**\n",
        "```\n",
        "import resnet\n",
        "\n",
        "# models are 101 152 200\n",
        "model = resnet.resnet101(   \n",
        "                num_classes=2,\n",
        "                shortcut_type='B',\n",
        "                sample_size=64,\n",
        "                sample_duration=32)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "###**Densenet 121, 169, 201, 264**\n",
        "```\n",
        "from densenet import generate_model\n",
        "\n",
        "# models are 121 169 201 264\n",
        "model = generate_model(model_depth = 264 , num_classes = 2) \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "```\n",
        "###**Loading Model Checkpoints**\n",
        "Finally, be sure to load the trained model weight file.\n",
        "```\n",
        "checkpoint_model = \"weight_3D.pth\"  # weight file location\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_model))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkXmS_tacAYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "depth, height, width = 32, 64, 64   # dimension for converting point cloud to voxels\n",
        "\n",
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/densenet264_ep_9_acc_90.317_mcc_0.761.pth\", \"weight_3D.pth\")\n",
        "checkpoint_model = \"weight_3D.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kduBYBuS_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import densenet\n",
        "\n",
        "model = generate_model(model_depth = 264 , num_classes = 2) # values are 121 169 201 264\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huyxjbv3bxkp",
        "colab_type": "text"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tTP9KlRbxEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = []\n",
        "filenames = np.array([])\n",
        "\n",
        "\n",
        "big_batch_size = 1024   # number of test data loaded at a time\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "for big_batch_no in range(math.ceil(test_len/big_batch_size)):\n",
        "\n",
        "  this_batch_len = big_batch_size\n",
        "  if (test_len - big_batch_size * big_batch_no) < big_batch_size:\n",
        "    this_batch_len = test_len - big_batch_size * big_batch_no\n",
        "\n",
        "\n",
        "  test_x = np.zeros((this_batch_len, 3, depth, height, width), dtype=np.float32)\n",
        "  test_yy = np.zeros(this_batch_len)\n",
        "\n",
        "\n",
        "  # Load one big batch\n",
        "  for i in tqdm(range(this_batch_len)):\n",
        "\n",
        "    original_idx = i + big_batch_size * big_batch_no\n",
        "\n",
        "    f = files[original_idx]\n",
        "    original_name = f.replace(\".mp4\", \"\")\n",
        "\n",
        "    test_x[i, :, :, :, :] = torch.load(path + original_name + \".pt\")\n",
        "    test_yy[i] = int(original_name)\n",
        "\n",
        "  # Test one big batch\n",
        "  test_y = torch.from_numpy(test_yy).int()\n",
        "  ## change the data types from here\n",
        "  test_x = torch.from_numpy(test_x).float()\n",
        "  #test_y = torch.from_numpy(test_y)\n",
        "  \n",
        "  test = torch.utils.data.TensorDataset(test_x, test_y)\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False, num_workers=4)\n",
        "\n",
        "  model.eval()\n",
        "  for i,(images,labels) in tqdm(enumerate(test_loader)):\n",
        "      #print(labels)\n",
        "      filenames = np.append(filenames, labels)\n",
        "      \n",
        "      images = images.view(-1,3,depth,height,width)\n",
        "      test = Variable(images.to(device), requires_grad=False)\n",
        "      labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        # Forward propagation\n",
        "        outputs = model(test)\n",
        "\n",
        "        # Get predictions from the maximum value\n",
        "        m=nn.Softmax()\n",
        "        predicted = m(outputs)[:,1]\n",
        "        print(predicted)\n",
        "        #print(f\"prediction size are {predicted.shape}\")\n",
        "        y_pred = np.append(y_pred, predicted.cpu().numpy())\n",
        "\n",
        "make_submission_file(filenames, y_pred)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}