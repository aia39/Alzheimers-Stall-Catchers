{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Pointcloud_training_resnet_densenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL27zWUkCsuu",
        "colab_type": "text"
      },
      "source": [
        "# **Alzheimer-Stall-Catchers-Point-Cloud**\n",
        "\n",
        "This notebook contains necessary code for training data explained in **Point Cloud Based Approach** using 3D convolutional models such as:\n",
        "- Resnet3D 18\n",
        "- Resnet3D 101, 152, 200\n",
        "- Densenet3D 121, 169, 201, 264"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjL-RPvWl6zQ",
        "colab_type": "text"
      },
      "source": [
        "### Test GPU\n",
        "\n",
        "This portion is to test available GPU on the machine. Torch models run on CUDA enabled devices, and it is recommended to use such a machine. In case of running the code on Google Colab, after connecting to a new session, be sure to test which GPU is provided for the session. **Tesla K80** is the slowest GPU that will take 5-6 times training time than **Tesla T4** or **Tesla P100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7eGOe9mJkE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG067iUUQPeR",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive \n",
        "\n",
        "**Skip this step if running on local machine**\n",
        "\n",
        "Running the notebook on Colab requires data files and custom python modules copied from google drive or uploading the files to the colab session. However due to large dataset sizes, uploading data on each session if not a viable option. It is recommended to upload the data on a google drive, and running the following cell, allow file access to that drive for easily copying necessary files.\n",
        "\n",
        "(Note that free google drive accounts give you only 15 GB of storage. In case data volume is bigger than that, you can create segments of the total data, hosting them on different drives, and then simply add shortcuts of the different drive folders to a single drive, giving you access to all the data from one google drive.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok8yK4RbDFNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Uh_vIJQZzs",
        "colab_type": "text"
      },
      "source": [
        "# Importing Necessary Libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwCqwra84SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef as mcc\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.manual_seed(100)\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKnyArwwc88j",
        "colab_type": "text"
      },
      "source": [
        "# Copy data\n",
        "\n",
        "**Skip this cell if running on local machine**\n",
        "\n",
        "After mounting google drive, you need to copy the necessary custom python modules, dataset and other files. The following cell first copies that large zipped data, and extracts them into the current colab workspace.\n",
        "\n",
        "**traintestlist.zip** contains the train-validation split we had previously created for experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HtLWl5BAWg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!jar -xf \"/content/drive/My Drive/Alzheimer/traintestlist.zip\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRHm31nAs-_",
        "colab_type": "text"
      },
      "source": [
        "If you have not yet converted the original dataset to the point cloud dataset yet, head to the <a href=\"https://github.com/ClockWorkKid/Alzheimers-Stall-Catchers/tree/master/Dataset%20Visualization%20and%20Processing\">**DATA VISUALIZATION AND PROCESSING**</a> section of the repository\n",
        "\n",
        "\n",
        "For importing dataset from google drive, the data has been partitioned into 3 zip files, and each of the files are extracted into the colab session one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAOia2rODiZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!jar -xf \"/content/drive/My Drive/Alzheimer/micro_1.zip\";\n",
        "print(\"partition 1 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/Alzheimer/micro_2.zip\";\n",
        "print(\"partition 2 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/Alzheimer/micro_3.zip\";\n",
        "print(\"partition 3 imported\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ptwJpFAHb8",
        "colab_type": "text"
      },
      "source": [
        "Alzheimer Stall Catchers **Micro** dataset contains 2399 data samples, and there is a sanity check to see whether all the data files have been imported successfully. If running on local machine, you can just specify the data folder to check if all files are there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX77HyTTASRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = [f for f in glob.glob(\"micro/\" + \"*\" + \".pt\", recursive=True)]\n",
        "print(\"Total: \" + str(len(files)) + \" should be 2399\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnCtAgKxOw5P",
        "colab_type": "text"
      },
      "source": [
        "You need to import custom modules for Resnet and Densenet 3D models. In case of local machine, simply copy these files to the notebook directory and head on to the next steps. In case of using Colab, copy the files from drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTu6xqZAPMdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/resnet.py\", \"resnet.py\"\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/densenet.py\", \"densenet.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AmKYzAR_5fD",
        "colab_type": "text"
      },
      "source": [
        "If you need to remove any folders from the colab workspace fully including any subdirectories and files, simply run the command:\n",
        "```\n",
        "! rm -rf <folder_name>\n",
        "```\n",
        "\n",
        "For example, running the following code removes the traintestlist folder from the session.\n",
        "```\n",
        "! rm -rf traintestlist\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEau83HIC5Dj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Torch deep learning models use dataloaders while importing data during training/testing passing them to CPU/GPU. Torch datasets can be easily modified to suit custom datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWjtb3jFxUli",
        "colab_type": "text"
      },
      "source": [
        "### Balanced Batch Sampler\n",
        "\n",
        "Balanced batch sampler can be used to create training batches from heavily imbalanced datasets (most biomedical datasets). The process is to oversample the data class with the lower instance count, so that each batch has 50:50 ratio while training. In each epoch, the model might see the same data from the lower count data class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQFDF7F5l2CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch.utils.data\n",
        "import random\n",
        "\n",
        "\n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = {}\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = []\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "    \n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        while len(self.dataset[self.keys[self.currentkey]]) > 0:\n",
        "            yield self.dataset[self.keys[self.currentkey]].pop()\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "\n",
        "    \n",
        "    def _get_label(self, dataset, idx):\n",
        "        dataset_type = type(dataset)\n",
        "        if dataset_type is torchvision.datasets.MNIST:\n",
        "            return dataset.train_labels[idx].item()\n",
        "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
        "            return dataset.imgs[idx][1]\n",
        "        else:\n",
        "            (image_sequence, target) = dataset.__getitem__(idx)\n",
        "            return target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8c05JU4xmkQ",
        "colab_type": "text"
      },
      "source": [
        "### Custom Dataset Class\n",
        "\n",
        "The dataloader here loads point cloud dataset saved in disk in .pt file format (torch tensor data).\n",
        "\n",
        "Here is a link from <a href=\"https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\">stanford.edu</a> that beautifully explains how to create your own custom dataloader for torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJqzMzjp3qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VoxelDataset(Dataset):\n",
        "    def __init__(self, dataset_path, split_path, split_number, training):\n",
        "        self.training = training\n",
        "        self.sequences, self.labels = self._extract_sequence_paths_and_labels(dataset_path, split_path, split_number,\n",
        "                                                                              training)  # creating a list of directories where the extracted frames are saved\n",
        "        self.label_names = [\"Non-stalled\", \"Stalled\"]  # Getting the label names or name of the class\n",
        "        self.num_classes = len(self.label_names)  # Getting the number of class\n",
        "\n",
        "\n",
        "    def _extract_sequence_paths_and_labels(\n",
        "            self, dataset_path, split_path=\"data/traintestlist\", split_number=0, training=True\n",
        "    ):\n",
        "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
        "        fn = f\"fold_{split_number}_train.csv\" if training else f\"fold_{split_number}_test.csv\"\n",
        "        split_path = os.path.join(split_path, fn)\n",
        "        df = pd.read_csv(split_path)\n",
        "        file_name = df['filename'].values\n",
        "        all_labels = df['class'].values\n",
        "        sequence_paths = []\n",
        "        classes = []\n",
        "        for i, video_name in enumerate(file_name):\n",
        "            seq_name = video_name.split(\".mp4\")[0]\n",
        "            sequence_paths += [os.path.join(dataset_path, seq_name).replace('\\\\', '/')]\n",
        "            classes += [all_labels[i]]\n",
        "        return sequence_paths, classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence_path = self.sequences[index % len(self)]\n",
        "        target = self.labels[index % len(self)]\n",
        "\n",
        "        X = torch.load(sequence_path + \".pt\")\n",
        "\n",
        "        return X, target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S_h254ep7ue",
        "colab_type": "text"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "Torch dataloaders usually include methods for data augmentation. In those methods, each data instance is given a random augmentation while creating a new data batch, i.e. each data point is slightly modified during each epoch. However in our approach, we wanted to feed all the possible augmentations in a single epoch, necessarily creating 16 copies of each data point and giving them a specific augmentation such as rotation along Z-axis, upside down rotation, mirroring and combination of these. From our tests despite training time linearly increasing with augmentation volume, it gave huge performance boost because of reduced overfitting in all of the training models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t-jWXy8p7Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(images, labels, augment_volume=2):\n",
        "\n",
        "  [instances, channel, depth, height, width] = images.shape\n",
        "\n",
        "  images_aug = torch.zeros((instances*augment_volume, channel, depth, height, width))\n",
        "  labels_aug = torch.zeros(instances*augment_volume)\n",
        "\n",
        "  for aug_type in range(augment_volume):\n",
        "    augmented = im_aug(images, aug_type)\n",
        "    \n",
        "    images_aug[aug_type*instances:(aug_type+1)*instances, :, :, :, :] = augmented\n",
        "    labels_aug[aug_type*instances:(aug_type+1)*instances] = labels\n",
        "\n",
        "  return images_aug.float(), labels_aug.long()\n",
        "\n",
        "\n",
        "\n",
        "def im_aug(images, aug_type):\n",
        "\n",
        "    if aug_type == 0:\n",
        "      return images\n",
        "\n",
        "    elif aug_type == 1:\n",
        "      return torch.rot90(images, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 2:\n",
        "      return torch.rot90(images, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 3:\n",
        "      return torch.rot90(images, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 4:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 5:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 6:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 7:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 8:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 9:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 10:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 11:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 12:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 13:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 14:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 15:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 3, [3,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijEVFBR9qKN3",
        "colab_type": "text"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "Confusion matrix is created for each epoch and saved to a folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yusYOMxqL2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conf_mat(cf, epoch, acc, mcc, y_true, train_loss, train_acc, test_loss, test_acc):\n",
        "  try:\n",
        "    plt.imshow(cf,cmap=plt.cm.RdYlGn,interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.title('Confusion Matrix without Normalization')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    tick_marks = np.arange(len(set(y_true))) # length of classes\n",
        "    class_labels = ['Non Stalled','Stalled']\n",
        "    tick_marks\n",
        "    plt.xticks(tick_marks,class_labels)\n",
        "    plt.yticks(tick_marks,class_labels)\n",
        "    # plotting text value inside cells\n",
        "    thresh = cf.max() / 2.\n",
        "    for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
        "        plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
        "    #plt.show()\n",
        "    os.makedirs(\"confusion_matrix\", exist_ok=True)\n",
        "    plt.savefig(f\"confusion_matrix/epoch{epoch}_accuracy{round(acc.item(),3)}_mcc_{round(mcc.item(),3)}.png\")\n",
        "    plt.close('all')\n",
        "\n",
        "    os.makedirs(\"loss_acc_curve\", exist_ok=True)\n",
        "\n",
        "    ##creating subplot for loss,acc\n",
        "    fig1, axs = plt.subplots(4,sharex=True,constrained_layout=True)\n",
        "    axs[0].plot(train_loss, color = \"red\") \n",
        "    axs[0].set_title(\"Train loss\")\n",
        "    axs[1].plot(train_acc, color = \"blue\") \n",
        "    axs[1].set_title(\"Train Accuracy\")\n",
        "    axs[2].plot(test_loss, color = \"green\")\n",
        "    axs[2].set_title(\"Test Loss\")\n",
        "    axs[3].plot(test_acc) \n",
        "    axs[3].set_title(\"Test Accuracy\")\n",
        "    #fig1.tight_layout()\n",
        "    #plt.show()\n",
        "    fig1.savefig(f\"loss_acc_curve/epoch{epoch}_accuracy{round(acc.item(),3)}_mcc_{round(mcc.item(),3)}.png\")\n",
        "    plt.close(fig1)\n",
        "  except:\n",
        "    print(\"MCC is 0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMEDRVRQxshP",
        "colab_type": "text"
      },
      "source": [
        "# Training/Validation Batch Creation using Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMP7nk43xKJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "split_number = 3\n",
        "\n",
        "augmentation_enabled = True     # Set this flag to False if augmentation is not desired\n",
        "augment_volume = 16             # How many times data will be augmented (min 1 max 16)\n",
        "\n",
        "# If augmentation is enabled, original batch size is reduced, otherwise the\n",
        "# data will not fit in the GPU memory\n",
        "if augmentation_enabled ==  True:\n",
        "  batch_size = int(batch_size/augment_volume)\n",
        "\n",
        "\n",
        "dataset_path = 'micro'          # Path to the dataset directory\n",
        "split_path = 'traintestlist'    # Path to the folder containing train test split csv files\n",
        "\n",
        "depth, height, width = 32, 64, 64   # Input data dimension\n",
        "\n",
        "\n",
        "# Define training set\n",
        "train_dataset_vox = VoxelDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    split_path=split_path,\n",
        "    split_number=split_number,\n",
        "    training=True,\n",
        ")\n",
        "# Not using balanced batch sampler, shuffle enabled\n",
        "train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,shuffle=True, num_workers=4)\n",
        "# Using balanced batch sampler\n",
        "# train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,sampler=BalancedBatchSampler(train_dataset_vox),shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# Define validation set\n",
        "test_dataset_vox = VoxelDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    split_path=split_path,\n",
        "    split_number=split_number,\n",
        "    training=False,\n",
        ")\n",
        "test_dataloader_vox = DataLoader(test_dataset_vox, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzI-gKJC5D2",
        "colab_type": "text"
      },
      "source": [
        "# **Network Model**\n",
        "\n",
        "As mentioned at the beginning of the notebook, Resnet3D/Densenet3D models have been used for training. The model must be imported and sent to device prior to training, and depending on which model you wish to train, you have to import that model either from torch libary (resnet3D 18) or our custorm python modules (resnet.py/densenet.py)\n",
        "\n",
        "###**Resnet3D 18**\n",
        "```\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "model = r3d_18(pretrained = False) # Change to true for a pretrained model \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.fc.out_features = 2\n",
        "```\n",
        "\n",
        "###**Resnet 101, 152, 200**\n",
        "```\n",
        "import resnet\n",
        "\n",
        "# models are 101 152 200\n",
        "model = resnet.resnet101(   \n",
        "                num_classes=2,\n",
        "                shortcut_type='B',\n",
        "                sample_size=64,\n",
        "                sample_duration=32)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "###**Densenet 121, 169, 201, 264**\n",
        "```\n",
        "from densenet import generate_model\n",
        "\n",
        "# models are 121 169 201 264\n",
        "model = generate_model(model_depth = 264 , num_classes = 2) \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "```\n",
        "###**Loading Model Checkpoints**\n",
        "Finally, if you are continuing the training form a previous model checkpoint/ some other pretrained weight files, be sure to load them into the model.\n",
        "```\n",
        "checkpoint_model = \"weight_3D.pth\"  # weight file location\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_model))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD_k_KqaLImM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from densenet import generate_model\n",
        "\n",
        "model = generate_model(model_depth = 264 , num_classes = 2) # values are 121 169 201 264\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_n_kiriRFa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer/densenet169_ep_11_acc_89.316_mcc_0.735.pth\", \"weight_3D.pth\")\n",
        "checkpoint_model = \"weight_3D.pth\"\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t5wBMnwUhF7",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function\n",
        "\n",
        "### Class Balance Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2MenQ1TfWXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n",
        "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
        "    weights = (1.0 - beta) / np.array(effective_num)\n",
        "    weights = weights / np.sum(weights) * no_of_classes\n",
        "\n",
        "    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n",
        "\n",
        "    weights = torch.tensor(weights).float().to(device)\n",
        "    weights = weights.unsqueeze(0)\n",
        "    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n",
        "    weights = weights.sum(1)\n",
        "    weights = weights.unsqueeze(1)\n",
        "    weights = weights.repeat(1,no_of_classes)\n",
        "\n",
        "    if loss_type == \"focal\":\n",
        "        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n",
        "    elif loss_type == \"sigmoid\":\n",
        "        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)\n",
        "    elif loss_type == \"softmax\":\n",
        "        pred = logits.softmax(dim = 1)\n",
        "        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n",
        "    return cb_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJpUcnfLfZA",
        "colab_type": "text"
      },
      "source": [
        "### Class Balance Loss Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbb6ji1UgJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_of_classes = 2\n",
        "beta = 0.9999\n",
        "gamma = 2.0\n",
        "samples_per_cls = [int(batch_size*0.7), batch_size - int(batch_size*0.7)]   #class sample number in one mini batch(For example we have 70:30 ratio data of mini batch_size of 50)\n",
        "loss_type = \"softmax\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXzAKC0Lk-D",
        "colab_type": "text"
      },
      "source": [
        "# Training Hyperparameters\n",
        "\n",
        "While training the models for 9-10 epochs, the validation MCC score tends to start growing again instead of convering. At that point, it is generally a good idea to stop training and increasing the **L2 regularization** value from 1e-4 to somewhere around 5e-4 or 8e-4. This helps in reducing overfitting the model. Again this might result in the model having a high training bias, which can be further improved by loweing the **learning rate** to 5e-4 or 2e-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFf3Bkb5UoFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4e-KlLzBza",
        "colab_type": "text"
      },
      "source": [
        "# Validation\n",
        "\n",
        "This function computes the validation set accuracy and MCC. If you are running on colab instead of a local machine, the following line can be modified to save the weight files from each epoch on google drive instead of the colab runtime, which saves you from losing training progress due to runtime disconnection.\n",
        "\n",
        "```\n",
        "torch.save(model.state_dict(), f\"model_checkpoints/ep_{epoch}_acc_{round(accuracy.item(),3)}_mcc_{round(mcc_score.item(),3)}.pth\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4getcCEuOjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(epoch,train_loss, train_acc, test_loss, test_acc):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  t_loss = 0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  \n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for images, labels in test_dataloader_vox:\n",
        "\n",
        "      y_true = np.append(y_true, labels.numpy())\n",
        "\n",
        "      images = images.view(-1,3,depth,height,width)\n",
        "      \n",
        "      test = Variable(images.to(device), requires_grad=False)\n",
        "      labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "      with torch.no_grad():\n",
        "      \n",
        "        # Forward propagation\n",
        "        outputs = model(test)\n",
        "        t_loss += CB_loss(labels, outputs, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n",
        "\n",
        "        # Get predictions from the maximum value\n",
        "        predicted = torch.max(outputs.data, 1)[1]\n",
        "        #print(f\"prediction size are {predicted.shape}\")\n",
        "        y_pred = np.append(y_pred, predicted.cpu().numpy())\n",
        "        # Total number of labels\n",
        "        total += len(labels)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "  model.train()\n",
        "  loss = t_loss.cpu().numpy() / float(total)\n",
        "  test_loss.append(loss)\n",
        "  accuracy = 100 * correct / float(total)\n",
        "  test_acc.append(accuracy)\n",
        "\n",
        "  print('TESTING ---> Epoch: {} Loss: {} Accuracy: {} %'.format(epoch, round(loss,3), round(accuracy.item(),3)))\n",
        "  mcc_score = mcc(y_true, y_pred)\n",
        "  print(f\"Validation MCC {round(mcc_score,4)}\")\n",
        "\n",
        "\n",
        "  global MCC_SCORE\n",
        "  if MCC_SCORE < mcc_score:\n",
        "      MCC_SCORE = mcc_score\n",
        "  os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
        "  try:\n",
        "    torch.save(model.state_dict(), f\"model_checkpoints/ep_{epoch}_acc_{round(accuracy.item(),3)}_mcc_{round(mcc_score.item(),3)}.pth\")\n",
        "  except:\n",
        "    pass\n",
        "  cf =confusion_matrix(y_true, y_pred)\n",
        "  #print(cf)\n",
        "  conf_mat(cf, epoch, accuracy, mcc_score, y_true, train_loss, train_acc, test_loss, test_acc)\n",
        "\n",
        "  return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddJpxiHvjiu",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbHWrT1C5Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRAINING code\n",
        "\n",
        "MCC_SCORE = -1\n",
        "\n",
        "\n",
        "train_loss = []   #to keep track of loss with respect to number of epoch \n",
        "test_loss = []\n",
        "iteration_list = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    #count = 1\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    for i, (images, labels) in enumerate(train_dataloader_vox):\n",
        "        correct = 0\n",
        "        \n",
        "        if augmentation_enabled == True:\n",
        "          images, labels = augment(images, labels, augment_volume)\n",
        "\n",
        "        images = images.view(-1,3,depth,height,width)\n",
        "\n",
        "\n",
        "        train_img = Variable(images.to(device), requires_grad=True)\n",
        "        labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(train_img)\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = CB_loss(labels, outputs, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n",
        "\n",
        "        \n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        predicted = torch.max(outputs.data, 1)[1]   \n",
        "        correct = (predicted == labels).sum()\n",
        "        total = len(labels)        \n",
        "        accuracy = 100 * correct / float(total)\n",
        "\n",
        "        accuracy_list = np.append(accuracy_list, accuracy.cpu().numpy())\n",
        "        loss_list = np.append(loss_list, loss.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    final_loss = np.mean(loss_list)\n",
        "    final_acc = np.mean(accuracy_list)\n",
        "    print(\" \")\n",
        "    print(f\"TRAINING ---> Epoch: {epoch} Loss: {round(final_loss,4)} Accuracy: {round(final_acc,4)}\")\n",
        "    train_loss.append(final_loss)\n",
        "    train_acc.append(final_acc)\n",
        "    gc.collect() \n",
        "    test_loss , test_acc = validation(epoch, train_loss, train_acc, test_loss, test_acc)\n",
        "    \n",
        "    train_dataset_vox = VoxelDataset(\n",
        "        dataset_path=dataset_path,\n",
        "        split_path=split_path,\n",
        "        split_number=split_number,\n",
        "        training=True,\n",
        "    )\n",
        "    train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,shuffle=True, num_workers=4)\n",
        "    #train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,sampler=BalancedBatchSampler(train_dataset_vox),shuffle=False, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbFB8mHlzMND",
        "colab_type": "text"
      },
      "source": [
        "**TIP:** This training could take several hours depending on how many epochs you chose. You will want to let this run as you sleep or go to work for the day, etc. However, Colab Cloud Service kicks you off it's VMs if you are idle for too long (30-90 mins).\n",
        "\n",
        "To avoid this hold (CTRL + SHIFT + i) at the same time to open up the inspector view on your browser.\n",
        "\n",
        "Paste the following code into your console window and hit **Enter**\n",
        "```\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document\n",
        "  .querySelector('#top-toolbar > colab-connect-button')\n",
        "  .shadowRoot.querySelector('#connect')\n",
        "  .click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    }
  ]
}