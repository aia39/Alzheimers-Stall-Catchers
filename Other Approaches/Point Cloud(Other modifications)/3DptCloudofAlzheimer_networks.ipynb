{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "3DptCloudofAlzheimer_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjL-RPvWl6zQ",
        "colab_type": "text"
      },
      "source": [
        "# Test GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7eGOe9mJkE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.current_device()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQzfTfHbHQF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG067iUUQPeR",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok8yK4RbDFNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9uz1q_pllbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Folder => \"Alzheimer Competition\"\n",
        "https://drive.google.com/drive/folders/1-NIobsSrpU5JyUfu0bqbuWZU32GVZ7-m?usp=sharing\n",
        "\n",
        "Folder => \"SayeedColab\"\n",
        "https://drive.google.com/drive/folders/1wnL3y-kltnGwsV9dhha6BbOI7UwF0YJW?usp=sharing\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Uh_vIJQZzs",
        "colab_type": "text"
      },
      "source": [
        "# Copy Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwCqwra84SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "824a1995-afb4-4ea4-c2d8-093271b3d40c"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "shutil.copyfile(\"/content/drive/My Drive/AlzheimerStallCatcher3DConvPointCloud/sampler.py\", \"sampler.py\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/AlzheimerStallCatcher3DConvPointCloud/ranger.py\", \"ranger.py\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/AlzheimerStallCatcher3DConvPointCloud/resnext.py\", \"resnext.py\")\n",
        "shutil.copyfile(\"/content/drive/My Drive/AlzheimerStallCatcher3DConvPointCloud/resnet.py\", \"resnet.py\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/AlzheimerStallCatcher3DConvPointCloud/traintestlist.zip\";\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAOia2rODiZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f7f3b110-2519-47d8-8432-3b273fe8bab5"
      },
      "source": [
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/micro_1.zip\";\n",
        "print(\"partition 1 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/micro_2.zip\";\n",
        "print(\"partition 2 imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/micro_3.zip\";\n",
        "print(\"partition 3 imported\")\n",
        "\n",
        "files = [f for f in glob.glob(\"micro/\" + \"*\" + \".pt\", recursive=True)]\n",
        "print(\"Total: \" + str(len(files)) + \" should be 2399\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "partition 1 imported\n",
            "partition 2 imported\n",
            "partition 3 imported\n",
            "Total: 2399 should be 2399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ofABr9VByi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NEW DATA\n",
        "\"\"\"\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/stall.zip\";\n",
        "print(\"new data stall imported\")\n",
        "\n",
        "!jar -xf \"/content/drive/My Drive/SayeedColab/Alzheimer Data/nonstall_0.zip\";\n",
        "print(\"new data nonstall_0 imported\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDPeoCa-e5GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf loss_acc_curve # remove any folder fully"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWfvjBHvQLLb",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqOgAV40C5DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef as mcc\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.manual_seed(100)\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEau83HIC5Dj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWjtb3jFxUli",
        "colab_type": "text"
      },
      "source": [
        "### Balanced Batch Sampler\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQFDF7F5l2CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch.utils.data\n",
        "import random\n",
        "\n",
        "\n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = {}\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = []\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "    \n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        while len(self.dataset[self.keys[self.currentkey]]) > 0:\n",
        "            yield self.dataset[self.keys[self.currentkey]].pop()\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "\n",
        "    \n",
        "    def _get_label(self, dataset, idx):\n",
        "        dataset_type = type(dataset)\n",
        "        if dataset_type is torchvision.datasets.MNIST:\n",
        "            return dataset.train_labels[idx].item()\n",
        "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
        "            return dataset.imgs[idx][1]\n",
        "        else:\n",
        "            (image_sequence, target) = dataset.__getitem__(idx)\n",
        "            return target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8c05JU4xmkQ",
        "colab_type": "text"
      },
      "source": [
        "### Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJqzMzjp3qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VoxelDataset(Dataset):\n",
        "    def __init__(self, dataset_path, split_path, split_number, training):\n",
        "        self.training = training\n",
        "        self.sequences, self.labels = self._extract_sequence_paths_and_labels(dataset_path, split_path, split_number,\n",
        "                                                                              training)  # creating a list of directories where the extracted frames are saved\n",
        "        self.label_names = [\"Non-stalled\", \"Stalled\"]  # Getting the label names or name of the class\n",
        "        self.num_classes = len(self.label_names)  # Getting the number of class\n",
        "\n",
        "\n",
        "    def _extract_sequence_paths_and_labels(\n",
        "            self, dataset_path, split_path=\"data/traintestlist\", split_number=0, training=True\n",
        "    ):\n",
        "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
        "        fn = f\"fold_{split_number}_train.csv\" if training else f\"fold_{split_number}_test.csv\"\n",
        "        split_path = os.path.join(split_path, fn)\n",
        "        df = pd.read_csv(split_path)\n",
        "        file_name = df['filename'].values\n",
        "        all_labels = df['class'].values\n",
        "        sequence_paths = []\n",
        "        classes = []\n",
        "        for i, video_name in enumerate(file_name):\n",
        "            seq_name = video_name.split(\".mp4\")[0]\n",
        "            sequence_paths += [os.path.join(dataset_path, seq_name).replace('\\\\', '/')]\n",
        "            classes += [all_labels[i]]\n",
        "        return sequence_paths, classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence_path = self.sequences[index % len(self)]\n",
        "        target = self.labels[index % len(self)]\n",
        "\n",
        "        X = torch.load(sequence_path + \".pt\")\n",
        "\n",
        "        return X, target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMEDRVRQxshP",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMP7nk43xKJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "split_number = 0\n",
        "\n",
        "augmentation_enabled = True\n",
        "augment_volume = 16\n",
        "\n",
        "\n",
        "if augmentation_enabled ==  True:\n",
        "  batch_size = int(batch_size/augment_volume)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "dataset_path = 'micro'\n",
        "split_path = 'traintestlist'\n",
        "\n",
        "depth, height, width = 32, 64, 64\n",
        "\n",
        "# Define training set\n",
        "\n",
        "train_dataset_vox = VoxelDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    split_path=split_path,\n",
        "    split_number=split_number,\n",
        "    training=True,\n",
        ")\n",
        "train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,shuffle=True, num_workers=4)\n",
        "#train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,sampler=BalancedBatchSampler(train_dataset_vox),shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# Define test set\n",
        "test_dataset_vox = VoxelDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    split_path=split_path,\n",
        "    split_number=split_number,\n",
        "    training=False,\n",
        ")\n",
        "test_dataloader_vox = DataLoader(test_dataset_vox, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "endtime = time.time()\n",
        "\n",
        "print(\"Elapsed time : \" + str(endtime-start))\n",
        "\n",
        "gc.collect() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzI-gKJC5D2",
        "colab_type": "text"
      },
      "source": [
        "# Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_n_kiriRFa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.copyfile(\"/content/drive/My Drive/Alzheimer Competition/lr1e-4_dec8e-4_90.484_mcc_0.766.pth\", \"weight_3D.pth\")\n",
        "checkpoint_model = \"weight_3D.pth\"\n",
        "\n",
        "# fold3_resnet101_aug16_32_64_64_acc_85.476_mcc_0.641.pth\n",
        "# lr1e-4_dec1e-3_acc_89.649_mcc_0.744.pth\n",
        "# lr1e-4_dec8e-4_90.484_mcc_0.766.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI0IKKUVP_22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import resnet\n",
        "model = resnet.resnet101(\n",
        "                num_classes=2,\n",
        "                shortcut_type='B',\n",
        "                sample_size=64,\n",
        "                sample_duration=32)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABT1AYlDdzvd",
        "colab_type": "text"
      },
      "source": [
        "### Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwWzMuOuB6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#error = nn.BCEWithLogitsLoss().to(device)\n",
        "error = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)  #added L2 regularization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4e-KlLzBza",
        "colab_type": "text"
      },
      "source": [
        "# Visualization and Saving Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNcBjSDXrvbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conf_mat(cf, epoch, acc, mcc, y_true, train_loss, train_acc, test_loss, test_acc):\n",
        "  try:\n",
        "    plt.imshow(cf,cmap=plt.cm.RdYlGn,interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.title('Confusion Matrix without Normalization')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    tick_marks = np.arange(len(set(y_true))) # length of classes\n",
        "    class_labels = ['Non Stalled','Stalled']\n",
        "    tick_marks\n",
        "    plt.xticks(tick_marks,class_labels)\n",
        "    plt.yticks(tick_marks,class_labels)\n",
        "    # plotting text value inside cells\n",
        "    thresh = cf.max() / 2.\n",
        "    for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
        "        plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
        "    #plt.show()\n",
        "    os.makedirs(\"confusion_matrix\", exist_ok=True)\n",
        "    plt.savefig(f\"confusion_matrix/epoch{epoch}_accuracy{round(acc.item(),3)}_mcc_{round(mcc.item(),3)}.png\")\n",
        "    plt.close('all')\n",
        "\n",
        "    os.makedirs(\"loss_acc_curve\", exist_ok=True)\n",
        "\n",
        "    ##creating subplot for loss,acc\n",
        "    fig1, axs = plt.subplots(4,sharex=True,constrained_layout=True)\n",
        "    axs[0].plot(train_loss, color = \"red\") \n",
        "    axs[0].set_title(\"Train loss\")\n",
        "    axs[1].plot(train_acc, color = \"blue\") \n",
        "    axs[1].set_title(\"Train Accuracy\")\n",
        "    axs[2].plot(test_loss, color = \"green\")\n",
        "    axs[2].set_title(\"Test Loss\")\n",
        "    axs[3].plot(test_acc) \n",
        "    axs[3].set_title(\"Test Accuracy\")\n",
        "    #fig1.tight_layout()\n",
        "    #plt.show()\n",
        "    fig1.savefig(f\"loss_acc_curve/epoch{epoch}_accuracy{round(acc.item(),3)}_mcc_{round(mcc.item(),3)}.png\")\n",
        "    plt.close(fig1)\n",
        "  except:\n",
        "    print(\"MCC is 0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4getcCEuOjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for cross entropy loss\n",
        "def validation(epoch,train_loss, train_acc, test_loss, test_acc):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  t_loss = 0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for images, labels in test_dataloader_vox:\n",
        "\n",
        "      y_true = np.append(y_true, labels.numpy())\n",
        "\n",
        "      images = images.view(-1,3,depth,height,width)\n",
        "\n",
        "      test = Variable(images.to(device), requires_grad=False)\n",
        "      labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "      with torch.no_grad():\n",
        "      \n",
        "        # Forward propagation\n",
        "        outputs = model(test)\n",
        "        t_loss += error(outputs, labels)\n",
        "        \n",
        "        # Get predictions from the maximum value\n",
        "        predicted = torch.max(outputs.data, 1)[1]\n",
        "        #print(f\"prediction size are {predicted.shape}\")\n",
        "        y_pred = np.append(y_pred, predicted.cpu().numpy())\n",
        "        # Total number of labels\n",
        "        total += len(labels)\n",
        "        correct += (predicted == labels).sum()\n",
        "  model.train()\n",
        "  loss = t_loss.cpu().numpy() / float(total)\n",
        "  test_loss.append(loss)\n",
        "  accuracy = 100 * correct / float(total)\n",
        "  test_acc.append(accuracy)\n",
        "\n",
        "  mcc_score = mcc(y_true, y_pred)\n",
        "  print(f\"MCC score is {round(mcc_score,4)}\")\n",
        "\n",
        "\n",
        "  global MCC_SCORE\n",
        "  if MCC_SCORE < mcc_score:\n",
        "      MCC_SCORE = mcc_score\n",
        "      os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
        "      try:\n",
        "        torch.save(model.state_dict(), f\"model_checkpoints/3D_pointcloud_SimpleCNN_{epoch}for_cloud_size_32_64_64_acc_{round(accuracy.item(),3)}_mcc_{round(mcc_score.item(),3)}.pth\")\n",
        "      except:\n",
        "        pass\n",
        "  cf =confusion_matrix(y_true, y_pred)\n",
        "  #print(cf)\n",
        "  conf_mat(cf, epoch, accuracy, mcc_score, y_true, train_loss, train_acc, test_loss, test_acc)\n",
        "\n",
        "  print('TESTING ---> Epoch: {} Loss: {} Accuracy: {} %'.format(epoch, round(loss,3), round(accuracy.item(),3)))\n",
        "\n",
        "\n",
        "  return test_loss, test_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddJpxiHvjiu",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6KGqof_2dKH",
        "colab_type": "text"
      },
      "source": [
        "**TIP:** This training could take several hours depending on how many iterations you chose in the .cfg file. You will want to let this run as you sleep or go to work for the day, etc. However, Colab Cloud Service kicks you off it's VMs if you are idle for too long (30-90 mins).\n",
        "\n",
        "To avoid this hold (CTRL + SHIFT + i) at the same time to open up the inspector view on your browser.\n",
        "\n",
        "Paste the following code into your console window and hit **Enter**\n",
        "```\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document\n",
        "  .querySelector('#top-toolbar > colab-connect-button')\n",
        "  .shadowRoot.querySelector('#connect')\n",
        "  .click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyBYPn3lHmd",
        "colab_type": "text"
      },
      "source": [
        "### Augmentation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1JkIye18KEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(images, labels, augment_volume=2):\n",
        "\n",
        "  [instances, channel, depth, height, width] = images.shape\n",
        "\n",
        "  images_aug = torch.zeros((instances*augment_volume, channel, depth, height, width))\n",
        "  labels_aug = torch.zeros(instances*augment_volume)\n",
        "\n",
        "  for aug_type in range(augment_volume):\n",
        "    augmented = im_aug(images, aug_type)\n",
        "    \n",
        "    images_aug[aug_type*instances:(aug_type+1)*instances, :, :, :, :] = augmented\n",
        "    labels_aug[aug_type*instances:(aug_type+1)*instances] = labels\n",
        "\n",
        "  return images_aug.float(), labels_aug.long()\n",
        "\n",
        "\n",
        "\n",
        "def im_aug(images, aug_type):\n",
        "\n",
        "    if aug_type == 0:\n",
        "      return images\n",
        "\n",
        "    elif aug_type == 1:\n",
        "      return torch.rot90(images, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 2:\n",
        "      return torch.rot90(images, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 3:\n",
        "      return torch.rot90(images, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 4:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 5:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 6:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 7:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      return torch.rot90(temp, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 8:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 9:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 10:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 11:\n",
        "      temp = torch.transpose(images, 3, 4)\n",
        "      return torch.rot90(temp, 3, [3,4])\n",
        "\n",
        "    elif aug_type == 12:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return temp\n",
        "\n",
        "    elif aug_type == 13:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 1, [3,4])\n",
        "\n",
        "    elif aug_type == 14:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 2, [3,4])\n",
        "\n",
        "    elif aug_type == 15:\n",
        "      temp = torch.flip(images, [2,3])\n",
        "      temp = torch.transpose(temp, 3, 4)\n",
        "      return torch.rot90(temp, 3, [3,4])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5dOXwdlK5W",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbHWrT1C5Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRAINING code\n",
        "\n",
        "MCC_SCORE = -1\n",
        "\n",
        "\n",
        "train_loss = []   #to keep track of loss with respect to number of epoch \n",
        "test_loss = []\n",
        "iteration_list = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    #count = 1\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    for i, (images, labels) in enumerate(train_dataloader_vox):\n",
        "        correct = 0\n",
        "        \n",
        "        if augmentation_enabled == True:\n",
        "          images, labels = augment(images, labels, augment_volume)\n",
        "\n",
        "        images = images.view(-1,3,depth,height,width)\n",
        "\n",
        "\n",
        "        #train = Variable(images)    #to use on cpu\n",
        "        #labels = Variable(labels)    #to use on cpu\n",
        "\n",
        "        train_img = Variable(images.to(device), requires_grad=True)\n",
        "        labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(train_img)\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, labels)\n",
        "        \n",
        "        #labels = labels.view(-1, 1)  #only for BCELogitsloss\n",
        "        #loss = error(outputs.float(), labels.float())  #if BCELoss is measured\n",
        "        \n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        predicted = torch.max(outputs.data, 1)[1]   \n",
        "        correct = (predicted == labels).sum()\n",
        "        total = len(labels)        \n",
        "        accuracy = 100 * correct / float(total)\n",
        "\n",
        "        '''\n",
        "        ###For BCELoss\n",
        "        predicted = torch.tensor([0 if i<=0.5 else 1 for i in outputs]).to(device)\n",
        "        accuracy = 100 * (predicted.detach() == labels).cpu().numpy().mean()\n",
        "        '''\n",
        "        #accuracy_list = np.append(accuracy_list, accuracy)\n",
        "\n",
        "        accuracy_list = np.append(accuracy_list, accuracy.cpu().numpy())\n",
        "        loss_list = np.append(loss_list, loss.detach().cpu().numpy())\n",
        "        #print(f\"Training Mini Batch--> Epoch:{epoch} Iteration:{count} Loss :{loss} Accuracy: {accuracy}\")\n",
        "        #count += 1\n",
        "    final_loss = np.mean(loss_list)\n",
        "    final_acc = np.mean(accuracy_list)\n",
        "    print(f\"TRAINING ---> Epoch: {epoch} Loss: {round(final_loss,4)} Accuracy: {round(final_acc,4)}\")\n",
        "    train_loss.append(final_loss)\n",
        "    train_acc.append(final_acc)\n",
        "    gc.collect() \n",
        "    test_loss , test_acc = validation(epoch, train_loss, train_acc, test_loss, test_acc)\n",
        "    \n",
        "    train_dataset_vox = VoxelDataset(\n",
        "        dataset_path=dataset_path,\n",
        "        split_path=split_path,\n",
        "        split_number=split_number,\n",
        "        training=True,\n",
        "    )\n",
        "    train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,shuffle=True, num_workers=4)\n",
        "    #train_dataloader_vox = DataLoader(train_dataset_vox, batch_size= batch_size,sampler=BalancedBatchSampler(train_dataset_vox),shuffle=False, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbFB8mHlzMND",
        "colab_type": "text"
      },
      "source": [
        "# Testing Validation Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WignzgdEctqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_aug(augment_enabled=False, augment_volume=16):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  t_loss = 0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for images, labels in test_dataloader_vox:\n",
        "\n",
        "      y_true = np.append(y_true, labels.numpy())\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        image_s = images.view(-1,3,depth,height,width)\n",
        "        test = Variable(image_s.to(device), requires_grad=False)\n",
        "        labels = Variable(labels.to(device), requires_grad=False)\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(test)\n",
        "\n",
        "        if (augment_enabled == True):\n",
        "\n",
        "            for aug_type in range(1, augment_volume):\n",
        "\n",
        "              image_s = im_aug(images, aug_type)\n",
        "              image_s = image_s.view(-1,3,depth,height,width)\n",
        "              test = Variable(image_s.to(device), requires_grad=False)\n",
        "              outputs = outputs + model(test)\n",
        "\n",
        "            outputs = outputs / augment_volume\n",
        "\n",
        "        t_loss += error(outputs, labels)\n",
        "        \n",
        "        # Get predictions from the maximum value\n",
        "        predicted = torch.max(outputs.data, 1)[1]\n",
        "        #print(f\"prediction size are {predicted.shape}\")\n",
        "        y_pred = np.append(y_pred, predicted.cpu().numpy())\n",
        "        # Total number of labels\n",
        "        total += len(labels)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  loss = t_loss.cpu().numpy() / float(total)\n",
        "  accuracy = 100 * correct / float(total)\n",
        "\n",
        "  mcc_score = mcc(y_true, y_pred)\n",
        "  print(f\"MCC score is {round(mcc_score,4)}\")\n",
        "\n",
        "  print('TESTING ---> Loss: {} Accuracy: {} %'.format(round(loss,3), round(accuracy.item(),3)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLxXTtNs43qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_aug(augment_enabled=False, augment_volume=augment_volume) \n",
        "validation_aug(augment_enabled=True, augment_volume=4) \n",
        "validation_aug(augment_enabled=True, augment_volume=8) \n",
        "validation_aug(augment_enabled=True, augment_volume=16) \n",
        "\n",
        "print(\"Done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}